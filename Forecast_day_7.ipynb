{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00aba1;\"> Requirements </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.model_selection import LeaveOneOut, train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import sys\n",
    "from numpy import shape\n",
    "sys.path.append('C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\Code')\n",
    "import functions as fc\n",
    "import importlib\n",
    "importlib.reload(fc)\n",
    "import ANN\n",
    "import csv\n",
    "importlib.reload(ANN)\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00aba1;\"> Data preprocessing </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload training set\n",
    "TRAIN_X = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\x_train.csv\", index_col=\"ID\")\n",
    "TRAIN_Y = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\y_train.csv\", index_col=\"ID\")\n",
    "\n",
    "#Upload testing set\n",
    "\n",
    "TEST_X = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\x_test.csv\", index_col=\"ID\")\n",
    "TEST_Y = pd.read_csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\y_test.csv\", index_col=\"ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create index and columns\n",
    "train_x_index = TRAIN_X.index\n",
    "train_x_columns = TRAIN_X.columns\n",
    "\n",
    "train_y_index = TRAIN_Y.index\n",
    "train_y_columns = TRAIN_Y.columns\n",
    "\n",
    "test_x_index = TEST_X.index\n",
    "test_x_columns = TEST_X.columns\n",
    "\n",
    "test_y_index = TEST_Y.index\n",
    "test_y_columns = TEST_Y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(TRAIN_X)\n",
    "train_x_scaled = pd.DataFrame(scaler_x.transform(TRAIN_X), index=train_x_index, columns=train_x_columns)\n",
    "\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(TRAIN_Y)\n",
    "train_y_scaled = pd.DataFrame(scaler_y.transform(TRAIN_Y),index=train_y_index,columns=train_y_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_scaled = pd.DataFrame(scaler_x.transform(TEST_X), index=test_x_index, columns=test_x_columns)\n",
    "\n",
    "test_y_scaled = pd.DataFrame(scaler_y.transform(TEST_Y),index=test_y_index,columns=test_y_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data into arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_as_array = TRAIN_Y.values.ravel()\n",
    "test_y_as_array = TEST_Y.values.ravel()\n",
    "\n",
    "train_y_scaled_as_array = train_y_scaled.values.ravel()\n",
    "test_y_scaled_as_array = test_y_scaled.values.ravel()\n",
    "\n",
    "train_x_as_array = TRAIN_X.values.ravel()\n",
    "test_x_as_array = TEST_X.values.ravel()\n",
    "\n",
    "train_x_scaled_as_array = train_x_scaled.values.ravel()\n",
    "test_x_scaled_as_array = test_x_scaled.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00aba1;\"> Feature Selection </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = fc.create_pca_features(n_components=10,\n",
    "                                      train_x_scaled=train_x_scaled,\n",
    "                                      columns=train_x_columns,\n",
    "                                      train_x=TRAIN_X)\n",
    "\n",
    "features_anova = fc.create_anova_features(k_features=26,\n",
    "                                          train_x=TRAIN_X,\n",
    "                                          train_y=TRAIN_Y,\n",
    "                                          train_x_columns=train_x_columns)\n",
    "\n",
    "features_lasso = fc.create_lasso_features(cv=5,\n",
    "                                          max_iter=10000,\n",
    "                                          train_x_scaled=train_x_scaled,\n",
    "                                          train_y_array=train_y_as_array,\n",
    "                                          train_x_columns=train_x_columns,\n",
    "                                          train_x=TRAIN_X)\n",
    "\n",
    "features_correlation = fc.create_correlation_features(train_x=TRAIN_X,\n",
    "                                                      correlation_threshold=0.6)\n",
    "\n",
    "features_rfe = fc.create_RFE_features(n_features_to_select=26,\n",
    "                                      train_x_scaled=train_x_scaled,\n",
    "                                      train_y_scaled_array=train_y_scaled_as_array,\n",
    "                                      train_x=TRAIN_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA\n",
    "# fc.plot_of_the_cumulative_sum_of_eigenvalues(train_x_scaled=train_x_scaled,\n",
    "#                                              type_of_experience=\"Regression\",\n",
    "#                                              number_of_case=2,\n",
    "#                                              save=\"y\")\n",
    "\n",
    "# # ANOVA\n",
    "# fc.plot_ANOVA_F_values(train_x=TRAIN_X,\n",
    "#                        train_y_array=train_y_as_array,\n",
    "#                        train_x_columns=train_x_columns,\n",
    "#                        type_of_experience=\"Regression\",\n",
    "#                        number_of_case=2,\n",
    "#                        save=\"y\")\n",
    "\n",
    "# # LASSO\n",
    "# fc.plot_lasso_coef_values(cv=5,\n",
    "#                           max_iter=10000,\n",
    "#                           train_x_scaled=train_x_scaled,\n",
    "#                           train_y_array=train_y_as_array,\n",
    "#                           train_x_columns=train_x_columns,\n",
    "#                           type_of_experience=\"Regression\",\n",
    "#                           number_of_case=2,\n",
    "#                           save=\"y\")\n",
    "\n",
    "# #RFE\n",
    "# fc.plot_RFE_ranking(train_x_columns=train_x_columns,\n",
    "#                     n_features_to_select=26,\n",
    "#                     train_x_scaled=train_x_scaled,\n",
    "#                     train_y_scaled_array=train_y_scaled_as_array,\n",
    "#                     type_of_experience=\"Regression\",\n",
    "#                     number_of_case=2,\n",
    "#                     save=\"y\"\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneOut()\n",
    "\n",
    "scoring = {\n",
    "    \"MSE\" : mean_squared_error,\n",
    "    \"MAE\": mean_absolute_error\n",
    "}\n",
    "\n",
    "features = {\n",
    "    \"features_pca\": features_pca,\n",
    "    \"features_anova\": features_anova,\n",
    "    \"features_lasso\": features_lasso,\n",
    "    \"features_correlation\": features_correlation,\n",
    "    \"features_rfe\": features_rfe,\n",
    "    \"all_features\": TRAIN_X\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00aba1;\"> Grid Search </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_train, x_val_test,y_val_train, y_val_test = train_test_split(TRAIN_X,TRAIN_Y,test_size=0.18,random_state=42)\n",
    "x_val_train_scaled, x_val_test_scaled,y_val_train_scaled, y_val_test_scaled = train_test_split(train_x_scaled,train_y_scaled,test_size=0.18,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_train_as_array = y_val_train.values.ravel()\n",
    "y_val_test_as_array = y_val_test.values.ravel()\n",
    "\n",
    "y_val_train_scaled_as_array = y_val_train_scaled.values.ravel()\n",
    "y_val_test_scaled_as_array = y_val_test_scaled.values.ravel()\n",
    "\n",
    "x_val_train_as_array = x_val_train.values.ravel()\n",
    "x_val_test_as_array = x_val_test.values.ravel()\n",
    "\n",
    "x_val_train_scaled_as_array = x_val_train_scaled.values.ravel()\n",
    "x_val_test_scaled_as_array = x_val_test_scaled.values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch for RF\n",
    "\n",
    "for subset in features:\n",
    "\n",
    "    features_selected = features[subset]\n",
    "\n",
    "    x = x_val_train[features_selected.columns]\n",
    "    x_val_test_for_search = x_val_test[features_selected.columns]\n",
    "\n",
    "    dict_search = fc.search_ML(\n",
    "        dict_param=fc.RFR_param,\n",
    "        algorithm=RandomForestRegressor,\n",
    "        x_val_train=x,\n",
    "        y_val_train=y_val_train_as_array, \n",
    "        x_val_test = x_val_test_for_search,\n",
    "        y_val_test = y_val_test,\n",
    "        train_y_index=train_y_index,\n",
    "        train_y_columns=train_y_columns,\n",
    "        scaler=scaler_y,\n",
    "        train_y_as_array=y_val_train_as_array,\n",
    "        scoring=scoring,\n",
    "        mode=\"Regression\",\n",
    "        subset= subset,\n",
    "        case=2,\n",
    "        regressor = \"Random Forest\",\n",
    "        scale=None,\n",
    "        checkpoint_file = f'checkpoint_{subset}.pkl'\n",
    "    )\n",
    "\n",
    "    output_file_path = f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\{subset}.csv\"\n",
    "    if dict_search:\n",
    "        print(f\"Writing results to {output_file_path}\")\n",
    "        with open(output_file_path, \"w\", newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"Parameters\", \"Scores\"])  \n",
    "            for key, values in dict_search.items():\n",
    "                w.writerow([key, values])\n",
    "    else:\n",
    "        print(f\"No data to write for subset {subset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Grid Search for SVR\n",
    "\n",
    "for subset in features:\n",
    "    features_selected = features[subset]\n",
    "    x = x_val_train_scaled[features_selected.columns]\n",
    "    x_val_test_for_search = x_val_test_scaled[features_selected.columns]\n",
    "    \n",
    "    # Iterate over each kernel and accumulate the results\n",
    "    for kernel in [\"poly\", \"rbf\", \"sigmoid\", \"linear\"]:\n",
    "        dict_search = fc.search_ML(\n",
    "            dict_param=fc.SVR_param[kernel],\n",
    "            algorithm=SVR,\n",
    "            x_val_train=x,\n",
    "            y_val_train=y_val_train_scaled_as_array, \n",
    "            x_val_test = x_val_test_for_search,\n",
    "            y_val_test = y_val_test_scaled,\n",
    "            train_y_index=y_val_test.index,\n",
    "            train_y_columns=y_val_test.columns,\n",
    "            scaler=scaler_y,\n",
    "            train_y_as_array=y_val_train_as_array,\n",
    "            scoring=scoring,\n",
    "            mode=\"Regression\",\n",
    "            subset = subset,\n",
    "            case = 2,\n",
    "            regressor = \"Support vector\",\n",
    "            scale=\"y\",\n",
    "            checkpoint_file = f'checkpoint_{subset}_SVM.pkl'\n",
    "        )\n",
    "\n",
    "    output_file_path = f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\{subset}.csv\"\n",
    "    if dict_search:\n",
    "        print(f\"Writing results to {output_file_path}\")\n",
    "        with open(output_file_path, \"w\", newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"Parameters\", \"Scores\"])  \n",
    "            for key, values in dict_search.items():\n",
    "                w.writerow([key, values])\n",
    "    else:\n",
    "        print(f\"No data to write for subset {subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search ANN\n",
    "NN_param = {\n",
    "    \"optimizer\": [\"adam\"],\n",
    "    \"loss\" : [\"mae\",\"mse\"],\n",
    "    \"activation\":[\"relu\"],\n",
    "    \"layers_vector\": [[64,32],\n",
    "                      [128,64],\n",
    "                      [128,64,32],\n",
    "                      [64,32,16],\n",
    "                      [128,64,32,16],\n",
    "                      [256,128,64,32],\n",
    "                      [64,32,16,4]],\n",
    "    \"output_layer\": [1,2,4,8],\n",
    "    \"batch_size\": [2,4,8]\n",
    "\n",
    "}\n",
    "\n",
    "for subset in features:\n",
    "\n",
    "    features_selected = features[subset]\n",
    "\n",
    "    x = x_val_train_scaled[features_selected.columns]\n",
    "    x_val_test_for_search = x_val_test_scaled[features_selected.columns]\n",
    "\n",
    "    grid = fc.stage_experiments(NN_param)\n",
    "\n",
    "    store = fc.load_checkpoint(f\"checkpoint_file_{subset}.pkl\")\n",
    "\n",
    "    print(\"Initial checkpoint data:\", store)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for experiment in grid:\n",
    "            args = [v[experiment[i]] for i, v in enumerate(NN_param.values())]\n",
    "            count += 1\n",
    "            print(f\"Experiment {count}/{len(grid)}: {args}\")\n",
    "            args_dict = {k: args[i] for i, k in enumerate(NN_param)}\n",
    "\n",
    "            if str(args_dict) in store:\n",
    "                print(f\"Skipping experiment {count}/{len(grid)} (already completed)\")\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    model = ANN.NeuralNetworkModel(input_shape= [x.shape[1]],\n",
    "                                                    **args_dict)\n",
    "                    model.train_model(train_x = x,\n",
    "                    train_y = y_val_train_scaled,\n",
    "                    validation_split = 0.1,\n",
    "                    verbose=0,\n",
    "                    epochs = 100)\n",
    "                \n",
    "                    model.load_model()\n",
    "                    args_str = fc.sanitize_args_dict(args_dict)\n",
    "                    model.save_model(case=2,subset=subset,args=args_str)\n",
    "                    y_val_test_pred_scaled = model.predict(test_x=x_val_test_for_search, verbose=1)\n",
    "                    y_val_test_pred_scaled = pd.DataFrame(y_val_test_pred_scaled) \n",
    "                    y_val_test_pred = pd.DataFrame(scaler_y.inverse_transform(y_val_test_pred_scaled))\n",
    "\n",
    "                    for metric in scoring:\n",
    "                        value = scoring[metric](y_val_test,y_val_test_pred)\n",
    "                        if str(args_dict) not in store:\n",
    "                            store[str(args_dict)] = []\n",
    "                        store[str(args_dict)].append(value)\n",
    "                    print(f\"Results for experiment {count}: {store[str(args_dict)]}\")\n",
    "                    fc.save_checkpoint(store, f\"checkpoint_file_{subset}_NN.pkl\")\n",
    "                except Exception as e:\n",
    "                    store[str(args_dict)] = []\n",
    "                    fc.save_checkpoint(store, f\"checkpoint_file_{subset}_NN.pkl\")\n",
    "                    print(f\"Error encountered with args: {args_dict}\")\n",
    "                    print(f\"Exception: {e}\")\n",
    "                    continue\n",
    "\n",
    "    output_file_path = f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\{subset}.csv\"\n",
    "    if store:\n",
    "        print(f\"Writing results to {output_file_path}\")\n",
    "        with open(output_file_path, \"w\", newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"Parameters\", \"Scores\"])  \n",
    "            for key, values in store.items():\n",
    "                w.writerow([key, values])\n",
    "    else:\n",
    "        print(f\"No data to write for subset {subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#00aba1;\"> Testing  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search results\n",
    "results = fc.transform_csv_into_dict(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support Vector\\\\all_features.csv\")\n",
    "\n",
    "fc.get_best_param(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = {\n",
    "    \"features_pca\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\features_pca_adam_mse_relu_[64, 32, 16, 4]_8_4.keras\",\n",
    "    \"features_anova\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\features_anova_adam_mse_relu_[128, 64]_4_4.keras\",\n",
    "    \"features_lasso\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\features_lasso_adam_mae_relu_[64, 32, 16]_1_2.keras\",\n",
    "    \"features_correlation\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\features_correlation_adam_mae_relu_[64, 32, 16]_2_4.keras\",\n",
    "    \"features_rfe\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\features_rfe_adam_mae_relu_[128, 64, 32]_2_4.keras\",\n",
    "    \"all_features\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Neural Network\\\\all_features_adam_mae_relu_[256, 128, 64, 32]_1_4.keras\"\n",
    "}\n",
    "\n",
    "\n",
    "random_forest = {\n",
    "    \"features_pca\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\features_pca_1_absolute_error_2_7_log2_False.pkl\",\n",
    "    \"features_anova\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\features_anova_1_absolute_error_None_5_sqrt_True.pkl\",\n",
    "    \"features_lasso\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\features_lasso_1_squared_error_None_3_sqrt_True.pkl\",\n",
    "    \"features_correlation\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\features_correlation_1_absolute_error_2_7_log2_False.pkl\",\n",
    "    \"features_rfe\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\features_rfe_1_absolute_error_2_2_sqrt_True.pkl\",\n",
    "    \"all_features\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Random Forest\\\\all_features_1_absolute_error_2_7_log2_False.pkl\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "support_vector = {\n",
    "    \"features_pca\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\features_pca_poly_2_0.001_0.001_75.pkl\",\n",
    "    \"features_anova\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\features_anova_poly_2_0.1_0.001_75.pkl\",\n",
    "    \"features_lasso\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\features_lasso_poly_4_0.001_auto_50.pkl\",\n",
    "    \"features_correlation\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\features_correlation_poly_3_0.1_0.01_1.pkl\",\n",
    "    \"features_rfe\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\features_rfe_linear_0.1_0.0001_10.pkl\",\n",
    "    \"all_features\": f\"C:\\\\Users\\\\Admin\\\\Desktop\\\\Tese\\\\47\\\\Experiences\\\\Regression\\\\Case_2\\\\GridSearch\\\\Support vector\\\\all_features_linear_0.001_0.1_10.pkl\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_RF = {}\n",
    "results_test_ANN = {}\n",
    "results_test_SVR = {}\n",
    "\n",
    "y_pred_RF_test = {}\n",
    "y_pred_ANN_test = {}\n",
    "y_pred_SVR_test = {}\n",
    "\n",
    "for subset in features:\n",
    "    features_selected = features[subset]\n",
    "    x_test = test_x_scaled[features_selected.columns]\n",
    "\n",
    "    model = fc.load_model(support_vector[subset])\n",
    "    y_pred_scaled = model.predict(x_test)\n",
    "\n",
    "    y_pred_scaled = pd.DataFrame(y_pred_scaled, index=test_y_index, columns=test_y_columns)\n",
    "    y_pred = pd.DataFrame(scaler_y.inverse_transform(y_pred_scaled), index=test_y_index, columns=test_y_columns)\n",
    "    y_pred = y_pred.values.ravel()\n",
    "    y_pred_SVR_test[subset] = y_pred\n",
    "\n",
    "    for metric in scoring:\n",
    "        value = fc.get_scores(y_true=test_y_as_array, y_pred=y_pred, metric=scoring[metric])\n",
    "        results_test_SVR[f\"{subset}_{metric}\"] = value\n",
    "\n",
    "for subset in features:\n",
    "    features_selected = features[subset]\n",
    "    x_test = TEST_X[features_selected.columns]\n",
    "\n",
    "    model = fc.load_model(random_forest[subset])\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    y_pred_RF_test[subset] = y_pred\n",
    "\n",
    "    for metric in scoring:\n",
    "        value = fc.get_scores(y_true=test_y_as_array, y_pred=y_pred, metric=scoring[metric])\n",
    "        results_test_RF[f\"{subset}_{metric}\"] = value\n",
    "\n",
    "for subset in features:\n",
    "    features_selected = features[subset]\n",
    "    x_test = test_x_scaled[features_selected.columns]\n",
    "\n",
    "    model = ANN.NeuralNetworkModel(input_shape= [x_test.shape[1]],\n",
    "                                   optimizer=\"adam\",\n",
    "                                   loss = \"mse\",\n",
    "                                   activation=\"relu\",\n",
    "                                   layers_vector=[128,64,32,16],\n",
    "                                   output_layer=10,\n",
    "                                   batch_size = 5,)\n",
    "    model.load_model(checkpoint_path=neural_network[subset])\n",
    "    y_pred_scaled = model.predict(x_test)\n",
    "    y_pred_scaled = pd.DataFrame(y_pred_scaled)\n",
    "    y_pred = pd.DataFrame(scaler_y.inverse_transform(y_pred_scaled))\n",
    "    y_pred = y_pred.values.ravel()\n",
    "    y_pred_ANN_test[subset] = y_pred\n",
    "\n",
    "    for metric in scoring:\n",
    "        value = fc.get_scores(y_true=test_y_as_array, y_pred=y_pred, metric=scoring[metric])\n",
    "        results_test_ANN[f\"{subset}_{metric}\"] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_plots(\n",
    "#     results_test_dict, \n",
    "#     true_y, \n",
    "#     target, \n",
    "#     pred_y, \n",
    "#     model, \n",
    "#     save=False, \n",
    "#     plots_to_generate=None, \n",
    "#     xlim=None, \n",
    "#     ylim=None\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Generate selected plots for model evaluation with customizable options.\n",
    "\n",
    "#     Parameters:\n",
    "#     - results_test_dict (dict): Dictionary containing test metrics (MSE, MAE, etc.) per feature subset.\n",
    "#     - true_y (array-like): Ground truth values.\n",
    "#     - target (str): Target variable name (e.g., 'Mel_4').\n",
    "#     - pred_y (array-like): Predicted values.\n",
    "#     - model (str): Name of the model (e.g., 'SVR', 'ANN', 'RF').\n",
    "#     - save (bool): Whether to save the plots.\n",
    "#     - plots_to_generate (list): List of plots to generate, options are: \n",
    "#         ['bar_chart', 'true_vs_pred', 'percentage_histogram', 'residual_histogram'].\n",
    "#     - xlim (tuple): Limits for x-axis in residual histogram (default: None).\n",
    "#     - ylim (tuple): Limits for y-axis in residual histogram (default: None).\n",
    "\n",
    "#     Returns:\n",
    "#     - None\n",
    "#     \"\"\"\n",
    "\n",
    "#     if plots_to_generate is None:\n",
    "#         plots_to_generate = ['bar_chart', 'true_vs_pred', 'percentage_histogram', 'residual_histogram']\n",
    "\n",
    "#     # Prepare metrics for bar chart if included\n",
    "#     if 'bar_chart' in plots_to_generate:\n",
    "#         results = {}\n",
    "#         filtered_metrics_dict = {k: v for k, v in results_test_dict.items() if 'explained_variance' not in k}\n",
    "\n",
    "#         for key, value in filtered_metrics_dict.items():\n",
    "#             feature, metric = key.rsplit(\"_\", 1)\n",
    "#             if feature not in results:\n",
    "#                 results[feature] = [None, None]\n",
    "#             if metric == \"MSE\":\n",
    "#                 results[feature][0] = value\n",
    "#             elif metric == \"MAE\":\n",
    "#                 results[feature][1] = value\n",
    "\n",
    "#         fc.bar_for_metrics(\n",
    "#             dictionary=results, \n",
    "#             first_label=\"MSE\", \n",
    "#             second_label=\"MAE\", \n",
    "#             y_label=f\"Feature Subsets for {model}\", \n",
    "#             save=save\n",
    "#         )\n",
    "\n",
    "#     # Generate true vs predicted plot if included\n",
    "#     if 'true_vs_pred' in plots_to_generate:\n",
    "#         fc.plot_y_true_pred(\n",
    "#             true_y=true_y, \n",
    "#             target=target, \n",
    "#             pred_y=pred_y, \n",
    "#             model=model, \n",
    "#             save=save\n",
    "#         )\n",
    "\n",
    "#     # Generate percentage histogram if included\n",
    "#     if 'percentage_histogram' in plots_to_generate:\n",
    "#         fc.percentage_histogram(\n",
    "#             true_y=true_y, \n",
    "#             target=target, \n",
    "#             pred_y=pred_y, \n",
    "#             model=model, \n",
    "#             save=save\n",
    "#         )\n",
    "\n",
    "#     # Generate residual histogram if included\n",
    "#     if 'residual_histogram' in plots_to_generate:\n",
    "#         fc.residual_histogram(\n",
    "#             true_y=true_y, \n",
    "#             target=target, \n",
    "#             pred_y=pred_y, \n",
    "#             model=model, \n",
    "#             xlim=xlim, \n",
    "#             ylim=ylim, \n",
    "#             save=save\n",
    "#         )\n",
    "\n",
    "\n",
    "# generate_plots(\n",
    "#     results_test_dict=results_test_SVR, \n",
    "#     true_y=TEST_Y, \n",
    "#     target=\"Mel_7\", \n",
    "#     pred_y=y_pred_SVR_test, \n",
    "#     model=\"SVR\", \n",
    "#     save=\"\", \n",
    "#     plots_to_generate=['bar_chart', 'true_vs_pred', 'residual_histogram',\"percentage_histogram\"], \n",
    "#     xlim=(-7, 10), \n",
    "#     ylim=(0, 6)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y_pred_RF_test \n",
    "y_pred_ANN_test \n",
    "# y_pred_SVR_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_diff_models ={\n",
    "    \"Random forest using LASSO features\":([ 9.75337768,  1.47899637, -0.5765469 , -2.32778455,  4.36842594,\n",
    "         5.73526332,  3.99796232, 35.69278525, 11.6508348 , 19.24383646]),\n",
    "    \"Neural network using ANOVA features\": ([15.644586 ,  3.6246595,  4.7745075,  6.7107615, 11.799844 ,\n",
    "         9.249467 , 10.7425995, 20.756308 ,  8.67581  , 14.93825  ]),\n",
    "    \"Support vector machine using PCA features\": ([4.30016082, 7.02508091, 6.96744992, 5.27361968, 6.89364922,\n",
    "        5.79509224, 6.71045266, 6.85101742, 6.77127458, 5.01115146])   \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.plot_y_true_pred(true_y=TEST_Y,target=\"Mel_7\",pred_y=y_pred_diff_models,model= \"\",save=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
